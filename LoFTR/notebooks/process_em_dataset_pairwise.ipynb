{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Electron Microscopy (EM) Dataset\n",
    "\n",
    "Processes the given EM dataset using LoFTR, creating a single, stitched image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from copy import deepcopy\n",
    "from src.loftr import LoFTR, default_cfg\n",
    "from src.utils.plotting import make_matching_figure\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = 0.05\n",
    "max_matches_shown = 50\n",
    "resolution_ratio = 0.5\n",
    "out_dir = \"out/TESCAN/8x3/\"\n",
    "target_dir = \"assets/TESCAN/8x3/\"\n",
    "\n",
    "# Check if the target directory exists.\n",
    "if not os.path.isdir(target_dir):\n",
    "    raise ValueError(f\"Cannot read directory: {target_dir}\")\n",
    "\n",
    "# Read the expected grid size.\n",
    "dir_name = os.path.basename(os.path.normpath(target_dir))\n",
    "match = re.match(r\"(\\d+)x(\\d+)\", dir_name, flags=re.IGNORECASE)\n",
    "if match is None:\n",
    "    raise ValueError(f\"Invalid grid specification: {dir_name}'\")\n",
    "grid_size = [int(x) for x in match.groups()]  # (rows, columns)\n",
    "\n",
    "# Pair adjacent images together.\n",
    "stitched_images = {}\n",
    "for path in sorted(os.listdir(target_dir)):\n",
    "    if os.path.isdir(path):\n",
    "        continue\n",
    "    \n",
    "    # Read grid, tile and slice info.\n",
    "    match = re.match(r\".*g(\\d+).t(\\d+).s(\\d+).*\", path, flags=re.IGNORECASE)\n",
    "    if match is None:\n",
    "        continue\n",
    "    tile_str = match.group(2)\n",
    "    grid_index, tile_index, slice_index = [int(x) for x in match.groups()]\n",
    "\n",
    "    # Pair with the below image if present.\n",
    "    new_pairs = []\n",
    "    if tile_index + grid_size[1] < grid_size[0] * grid_size[1]:\n",
    "        paired_tile_index = tile_index + grid_size[1]\n",
    "        paired = path.replace(\"t\" + tile_str,\n",
    "                              f\"t{paired_tile_index:0{len(tile_str)}d}\")\n",
    "        new_pairs.append({\"first\": path, \"second\": paired, \"type\": \"row\"})\n",
    "    \n",
    "    # Pair with the right image if present.\n",
    "    if (tile_index + 1) % grid_size[1] != 0:\n",
    "        paired_tile_index = tile_index + 1\n",
    "        paired = path.replace(\"t\" + tile_str,\n",
    "                              f\"t{paired_tile_index:0{len(tile_str)}d}\")\n",
    "        new_pairs.append({\"first\": path, \"second\": paired, \"type\": \"column\"})\n",
    "\n",
    "    # Check that all paired files exist.\n",
    "    for pair in new_pairs:\n",
    "        if not os.path.isfile(pair[\"second\"]):\n",
    "            ValueError(f\"Expected file in grid but none found: {pair['second']}\")\n",
    "    \n",
    "    # Save new pairs under the corresponding grid index.\n",
    "    if grid_index in stitched_images:\n",
    "        stitched_images[grid_index] = stitched_images[grid_index] + new_pairs\n",
    "    else:\n",
    "        stitched_images[grid_index] = new_pairs\n",
    "\n",
    "stitched_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare LoFTR for matching using indoor weights (better than outdoor for EM).\n",
    "_default_cfg = deepcopy(default_cfg)\n",
    "_default_cfg[\"coarse\"][\"temp_bug_fix\"] = True  # Temporary bugfix for the indoor checkpoint.\n",
    "matcher = LoFTR(config=_default_cfg)\n",
    "checkpoint_path = \"weights/indoor_ds.ckpt\"\n",
    "matcher.load_state_dict(torch.load(checkpoint_path)[\"state_dict\"])\n",
    "matcher = matcher.eval().cuda()\n",
    "\n",
    "# Match all paired images and save match visualisations.\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "for grid_index, stitched_pairs in stitched_images.items():\n",
    "    # Go over all stitched image pairs for the current grid segment.\n",
    "    for pair in stitched_pairs:\n",
    "        # Read both images.\n",
    "        path1 = os.path.join(target_dir, pair[\"first\"])\n",
    "        path2 = os.path.join(target_dir, pair[\"second\"])\n",
    "        img1 = cv2.imread(path1, cv2.IMREAD_GRAYSCALE)\n",
    "        img2 = cv2.imread(path2, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Check that both images are valid.\n",
    "        if img1 is None or img2 is None:\n",
    "            raise ValueError(f\"Failed to read image pair: ({pair['first']}, {pair['second']})\")\n",
    "        \n",
    "        # Crop both images based on expected overlap. Also rotate row pairs to\n",
    "        # a vertical position (required for precise matching).\n",
    "        height1, width1 = img1.shape\n",
    "        height2, width2 = img2.shape\n",
    "        if pair[\"type\"] == \"row\":\n",
    "            height_crop_range = int(height1 * overlap)\n",
    "            img1 = img1[-height_crop_range:, :]  # Top side\n",
    "            img2 = img2[:height_crop_range, :]  # Bottom side\n",
    "\n",
    "            # Rotate the images.\n",
    "            img1 = cv2.rotate(img1, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "            img2 = cv2.rotate(img2, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "        else:\n",
    "            width_crop_range = int(width1 * overlap)\n",
    "            img1 = img1[:, -width_crop_range:]  # Left side\n",
    "            img2 = img2[:, :width_crop_range]  # Right side\n",
    "\n",
    "        # Resize the images. Note that the final resolution needs to be\n",
    "        # adjusted to a multiple of 8.\n",
    "        height1, width1 = img1.shape\n",
    "        height2, width2 = img2.shape\n",
    "        img1 = cv2.resize(img1, (int(width1 * resolution_ratio) // 8 * 8,\n",
    "                                 int(height1 * resolution_ratio) // 8 * 8))\n",
    "        img2 = cv2.resize(img2, (int(width2 * resolution_ratio) // 8 * 8,\n",
    "                                 int(height2 * resolution_ratio) // 8 * 8))\n",
    "        \n",
    "        # Create float batch tensors.\n",
    "        batch_img1 = torch.from_numpy(img1)\n",
    "        batch_img1 = batch_img1.reshape(1, 1, *batch_img1.shape).cuda() / 255.0\n",
    "        batch_img2 = torch.from_numpy(img2)\n",
    "        batch_img2 = batch_img2.reshape(1, 1, *batch_img2.shape).cuda() / 255.0\n",
    "\n",
    "        # Run inference with LoFTR and get the prediction.\n",
    "        batch = {\"image0\": batch_img1, \"image1\": batch_img2}\n",
    "        with torch.no_grad():\n",
    "            matcher(batch)\n",
    "            matches1 = batch[\"mkpts0_f\"].cpu().numpy()\n",
    "            matches2 = batch[\"mkpts1_f\"].cpu().numpy()\n",
    "            confidence = batch[\"mconf\"].cpu().numpy()\n",
    "\n",
    "        # Save the matching figure.\n",
    "        match_samples = random.sample(range(0, len(matches1)),\n",
    "                                      min(max_matches_shown, len(matches1)))\n",
    "        color = cm.jet(confidence[match_samples])\n",
    "        text = [f\"Total Matches: {len(matches1)}\"]\n",
    "        fig = make_matching_figure(img1, img2, matches1[match_samples],\n",
    "                                   matches2[match_samples], color, text=text)\n",
    "        out_path = os.path.join(out_dir, f\"g{grid_index:04d}\",\n",
    "                                f\"{pair['first']}_{pair['second']}_matches.png\")\n",
    "        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "        fig.savefig(out_path)\n",
    "        plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('loftr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "15fd3888e566f887e83688ea48e046ac68e63c06e0e6ef05835d05dd2567edd5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
